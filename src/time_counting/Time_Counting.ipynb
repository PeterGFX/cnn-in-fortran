{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c98317-cdeb-4fb7-ae50-47cdb6afb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "from models import UNet\n",
    "from dataloader import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c89595b1-8e6c-4afd-aaf4-4712eaa2dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.528593 sec\n",
      "Model load time: 0.036943 sec\n",
      "Average inference (100 runs): 0.002110 sec\n",
      "Throughput: 473.97 samples/sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.502487 sec\n",
      "Model load time: 0.029973 sec\n",
      "Average inference (100 runs): 0.002122 sec\n",
      "Throughput: 471.32 samples/sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.594076 sec\n",
      "Model load time: 0.030573 sec\n",
      "Average inference (100 runs): 0.002126 sec\n",
      "Throughput: 470.45 samples/sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.578383 sec\n",
      "Model load time: 0.029953 sec\n",
      "Average inference (100 runs): 0.002120 sec\n",
      "Throughput: 471.77 samples/sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.538334 sec\n",
      "Model load time: 0.029346 sec\n",
      "Average inference (100 runs): 0.002131 sec\n",
      "Throughput: 469.33 samples/sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    timings = {}\n",
    "    in_len = 5\n",
    "    out_len = 1\n",
    "    \n",
    "    # Data preparation\n",
    "    torch.cuda.synchronize() \n",
    "    data_prep_start = time.time()\n",
    "    test_loader, _ = get_dataloaders(1,\n",
    "                                in_len=in_len,\n",
    "                                out_len=out_len,\n",
    "                                )\n",
    "    \n",
    "    test_input, test_target = next(iter(test_loader))\n",
    "    test_input, test_target = test_input.to(device), test_target.to(device).float()\n",
    "    torch.cuda.synchronize()\n",
    "    data_prep_end = time.time()\n",
    "    timings['data_preparation'] = data_prep_end - data_prep_start\n",
    "    \n",
    "    # Model load \n",
    "    torch.cuda.synchronize() \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        model = torch.jit.load('./checkpoint/unet_d5_out1_gpu_L1_traced.pt', map_location=\"cuda\")\n",
    "        model.eval()\n",
    "        torch.cuda.synchronize() \n",
    "        model_prep_end = time.time()\n",
    "        timings['model_load'] = model_prep_end - start_time\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_input)\n",
    "    \n",
    "    # Inference\n",
    "    num_runs = 100 \n",
    "    inference_times = []\n",
    "    \n",
    "    for j in range(num_runs):\n",
    "        torch.cuda.synchronize() \n",
    "        start_inference = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = model(test_input)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_inference = time.time()\n",
    "        inference_times.append(end_inference - start_inference)\n",
    "    \n",
    "    avg_inference = sum(inference_times) / num_runs\n",
    "    std_inference = (sum((x - avg_inference) ** 2 for x in inference_times) / num_runs) ** 0.5\n",
    "    timings['avg_inference'] = avg_inference\n",
    "    timings['std_inference'] = std_inference\n",
    "    \n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Data preparation time: {timings['data_preparation']:.6f} sec\")\n",
    "    print(f\"Model load time: {timings['model_load']:.6f} sec\")\n",
    "    print(f\"Average inference ({num_runs} runs): {avg_inference:.6f} sec\")\n",
    "    \n",
    "    batch_size = test_input.size(0)\n",
    "    throughput = batch_size / avg_inference\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9829935-4fdc-46fe-907b-8ff6b914f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.001819 sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.001367 sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.001186 sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.001170 sec\n",
      "Using device: cuda\n",
      "\n",
      "=== Result ===\n",
      "device: cuda\n",
      "Data preparation time: 0.001458 sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    timings = {}\n",
    "    in_len = 5\n",
    "    out_len = 1\n",
    "    \n",
    "    # Data preparation\n",
    "    torch.cuda.synchronize() \n",
    "    data_prep_start = time.time()\n",
    "    test_input = np.fromfile(\"data/input_tensor.dat\", dtype=np.float32)\n",
    "    input_shape = (320, 320, in_len, out_len)\n",
    "    test_input = test_input.reshape(input_shape)\n",
    "    test_input = test_input.transpose()\n",
    "    test_target = np.fromfile(\"data/target_tensor.dat\", dtype=np.float32)\n",
    "    target_shape = (320, 320, 1, 1)\n",
    "    test_target = test_target.reshape(target_shape)\n",
    "    test_target = test_target.transpose()\n",
    "\n",
    "    test_input, test_target = torch.from_numpy(test_input), torch.from_numpy(test_target) \n",
    "    test_input, test_target = test_input.to(device), test_target.to(device).float()\n",
    "    torch.cuda.synchronize()\n",
    "    data_prep_end = time.time()\n",
    "    timings['data_preparation'] = data_prep_end - data_prep_start\n",
    "       \n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Data preparation time: {timings['data_preparation']:.6f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5210248-0c88-4816-bdd7-786c9a62f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.525012 sec\n",
      "Model load time: 0.025642 sec\n",
      "Average inference (100 runs): 0.189187 sec\n",
      "Throughput: 5.29 samples/sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.644415 sec\n",
      "Model load time: 0.031176 sec\n",
      "Average inference (100 runs): 0.201618 sec\n",
      "Throughput: 4.96 samples/sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.577001 sec\n",
      "Model load time: 0.029350 sec\n",
      "Average inference (100 runs): 0.176266 sec\n",
      "Throughput: 5.67 samples/sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.589272 sec\n",
      "Model load time: 0.032431 sec\n",
      "Average inference (100 runs): 0.179104 sec\n",
      "Throughput: 5.58 samples/sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.749024 sec\n",
      "Model load time: 0.032046 sec\n",
      "Average inference (100 runs): 0.202761 sec\n",
      "Throughput: 4.93 samples/sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # set the device\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    timings = {}\n",
    "    in_len = 5\n",
    "    out_len = 1\n",
    "    \n",
    "    # Data preparation\n",
    "    data_prep_start = time.time()\n",
    "    test_loader, _ = get_dataloaders(1,\n",
    "                                in_len=in_len,\n",
    "                                out_len=out_len,\n",
    "                                )\n",
    "    \n",
    "    test_input, test_target = next(iter(test_loader))\n",
    "    test_input, test_target = test_input.to(device), test_target.to(device).float()\n",
    "    data_prep_end = time.time()\n",
    "    timings['data_preparation'] = data_prep_end - data_prep_start\n",
    "    \n",
    "    # Model load\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        model = torch.jit.load('./checkpoint/unet_d5_out1_gpu_L1_traced.pt', map_location=\"cpu\")\n",
    "        model.eval()\n",
    "        model_prep_end = time.time()\n",
    "        timings['model_load'] = model_prep_end - start_time\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_input)\n",
    "    \n",
    "    # Inference\n",
    "    num_runs = 100 \n",
    "    inference_times = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start_inference = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_output = model(test_input)\n",
    "        \n",
    "        end_inference = time.time()\n",
    "        inference_times.append(end_inference - start_inference)\n",
    "    \n",
    "    avg_inference = sum(inference_times) / num_runs\n",
    "    std_inference = (sum((x - avg_inference) ** 2 for x in inference_times) / num_runs) ** 0.5\n",
    "    timings['avg_inference'] = avg_inference\n",
    "    timings['std_inference'] = std_inference\n",
    "    \n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Data preparation time: {timings['data_preparation']:.6f} sec\")\n",
    "    print(f\"Model load time: {timings['model_load']:.6f} sec\")\n",
    "    print(f\"Average inference ({num_runs} runs): {avg_inference:.6f} sec\")\n",
    "    \n",
    "    batch_size = test_input.size(0)\n",
    "    throughput = batch_size / avg_inference\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239b69c1-f39d-46e9-9ea9-bea4ca1d60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.001596 sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.001217 sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.001234 sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.001185 sec\n",
      "Using device: cpu\n",
      "\n",
      "=== Result ===\n",
      "device: cpu\n",
      "Data preparation time: 0.001256 sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # set the device\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    timings = {}\n",
    "    in_len = 5\n",
    "    out_len = 1\n",
    "    \n",
    "    # Data preparation\n",
    "    data_prep_start = time.time()\n",
    "    test_input = np.fromfile(\"data/input_tensor.dat\", dtype=np.float32)\n",
    "    input_shape = (320, 320, in_len, out_len)\n",
    "    test_input = test_input.reshape(input_shape)\n",
    "    test_input = test_input.transpose()\n",
    "    test_target = np.fromfile(\"data/target_tensor.dat\", dtype=np.float32)\n",
    "    target_shape = (320, 320, 1, 1)\n",
    "    test_target = test_target.reshape(target_shape)\n",
    "    test_target = test_target.transpose()\n",
    "\n",
    "    test_input, test_target = torch.from_numpy(test_input), torch.from_numpy(test_target) \n",
    "    test_input, test_target = test_input.to(device), test_target.to(device).float()\n",
    "    data_prep_end = time.time()\n",
    "    timings['data_preparation'] = data_prep_end - data_prep_start\n",
    "    \n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Data preparation time: {timings['data_preparation']:.6f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d528c0-3440-41ee-ba17-b78c319df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc -c cuda_sync.c -o cuda_sync.o -I/user-environment/env/default/include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525d2861-61cf-4370-9d1e-24775071b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfortran -I/users/class191/FTorchbin/include -I/users/class191/FTorchbin/include/ftorch -I/users/class191/Project/FTorch/build/modules -o infer_fortran_cuda.x infer_fortran_cuda.f90 cuda_sync.o -L/users/class191/FTorchbin/lib64 -L/user-environment/env/default/lib64 -lftorch -lcudart\n",
      "LD_LIBRARY_PATH=/user-environment/env/default/lib64:$LD_LIBRARY_PATH ./infer_fortran_cuda.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: /users/class191/miniconda3/envs/weather-cnn/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CUDA\n",
      "  Data preparation time:    2.00000009E-03  sec\n",
      "  Model load time:          5.99999987E-02  sec\n",
      "  Average inference (         100  runs):    2.10000062E-03  sec\n",
      "  Throughput:              476.190338      samples/sec\n",
      " UNet inference ran successfully\n",
      "LD_LIBRARY_PATH=/user-environment/env/default/lib64:$LD_LIBRARY_PATH ./infer_fortran_cuda.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: /users/class191/miniconda3/envs/weather-cnn/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CUDA\n",
      "  Data preparation time:    3.00000003E-03  sec\n",
      "  Model load time:          5.79999983E-02  sec\n",
      "  Average inference (         100  runs):    2.11000093E-03  sec\n",
      "  Throughput:              473.933441      samples/sec\n",
      " UNet inference ran successfully\n",
      "LD_LIBRARY_PATH=/user-environment/env/default/lib64:$LD_LIBRARY_PATH ./infer_fortran_cuda.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: /users/class191/miniconda3/envs/weather-cnn/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CUDA\n",
      "  Data preparation time:    2.00000009E-03  sec\n",
      "  Model load time:          5.70000000E-02  sec\n",
      "  Average inference (         100  runs):    2.12000078E-03  sec\n",
      "  Throughput:              471.697937      samples/sec\n",
      " UNet inference ran successfully\n",
      "LD_LIBRARY_PATH=/user-environment/env/default/lib64:$LD_LIBRARY_PATH ./infer_fortran_cuda.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: /users/class191/miniconda3/envs/weather-cnn/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CUDA\n",
      "  Data preparation time:    2.00000009E-03  sec\n",
      "  Model load time:          5.70000000E-02  sec\n",
      "  Average inference (         100  runs):    2.10000062E-03  sec\n",
      "  Throughput:              476.190338      samples/sec\n",
      " UNet inference ran successfully\n",
      "LD_LIBRARY_PATH=/user-environment/env/default/lib64:$LD_LIBRARY_PATH ./infer_fortran_cuda.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: /users/class191/miniconda3/envs/weather-cnn/lib/libtinfo.so.6: no version information available (required by /lib64/libreadline.so.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CUDA\n",
      "  Data preparation time:    2.00000009E-03  sec\n",
      "  Model load time:          5.70000000E-02  sec\n",
      "  Average inference (         100  runs):    2.10000062E-03  sec\n",
      "  Throughput:              476.190338      samples/sec\n",
      " UNet inference ran successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export LD_LIBRARY_PATH=/users/class191/miniconda3/envs/weather-cnn/lib:/users/class191/FTorchbin/lib64:$LD_LIBRARY_PATH\n",
    "for i in {1..5}; do make run_infer_cuda MODEL=checkpoint/unet_d5_out1_gpu_L1_traced.pt DATADIR=data; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109c51ec-4883-4bfa-913a-222e02b7f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfortran -I/users/class191/FTorchbin/include -I/users/class191/FTorchbin/include/ftorch -I/users/class191/Project/FTorch/build/modules -o infer_fortran_cpu.x infer_fortran_cpu.f90 -L/users/class191/FTorchbin/lib64 -L/user-environment/env/default/lib64 -lftorch -lcudart\n",
      "./infer_fortran_cpu.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n",
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CPU\n",
      "  Data preparation time:    1.00000005E-03  sec\n",
      "  Model load time:         0.195999995      sec\n",
      "  Average inference (         100  runs):   0.305449963      sec\n",
      "  Throughput:              3.27385855      samples/sec\n",
      " UNet inference ran successfully\n",
      "./infer_fortran_cpu.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n",
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CPU\n",
      "  Data preparation time:    2.00000009E-03  sec\n",
      "  Model load time:          6.59999996E-02  sec\n",
      "  Average inference (         100  runs):   0.186890006      sec\n",
      "  Throughput:              5.35074091      samples/sec\n",
      " UNet inference ran successfully\n",
      "./infer_fortran_cpu.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n",
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CPU\n",
      "  Data preparation time:    1.00000005E-03  sec\n",
      "  Model load time:         0.136000007      sec\n",
      "  Average inference (         100  runs):   0.167170003      sec\n",
      "  Throughput:              5.98193455      samples/sec\n",
      " UNet inference ran successfully\n",
      "./infer_fortran_cpu.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n",
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CPU\n",
      "  Data preparation time:    1.00000005E-03  sec\n",
      "  Model load time:          8.29999968E-02  sec\n",
      "  Average inference (         100  runs):   0.191060007      sec\n",
      "  Throughput:              5.23395777      samples/sec\n",
      " UNet inference ran successfully\n",
      "./infer_fortran_cpu.x checkpoint/unet_d5_out1_gpu_L1_traced.pt data\n",
      " === Fortran (ftorch) Performance Results ===\n",
      " Device: CPU\n",
      "  Data preparation time:    1.00000005E-03  sec\n",
      "  Model load time:         0.101000004      sec\n",
      "  Average inference (         100  runs):   0.224119991      sec\n",
      "  Throughput:              4.46189547      samples/sec\n",
      " UNet inference ran successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export LD_LIBRARY_PATH=/users/class191/miniconda3/envs/weather-cnn/lib:/users/class191/FTorchbin/lib64:$LD_LIBRARY_PATH\n",
    "for i in {1..5}; do make run_infer_cpu MODEL=checkpoint/unet_d5_out1_gpu_L1_traced.pt DATADIR=data; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8df726-3c61-485b-ba4c-a13da84f8009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather-cnn",
   "language": "python",
   "name": "weather-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
